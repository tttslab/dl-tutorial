{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjnsOpbPPDQl"
      },
      "source": [
        "2023 Takahiro Shinozaki @ Tokyo Tech\n",
        "\n",
        "Contributors: Ryota Komatsu, Zhou Zehua, Tingyuan Zhu, Kota Kawakita\n",
        "\n",
        "Notebook for Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9r3onYR4_Pd"
      },
      "source": [
        "\n",
        "Learning Task:\n",
        "\n",
        "We (i.e., the environment) show a picture to the agent. The picture contains one to three food objects. If the agent correctly answers the number of objects, it gets a positive reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kjbKQB9da1f"
      },
      "source": [
        "Agent:\n",
        "We use Deep Q-Network (DQN) to implement the agent. DQN is a Q-learning method, which is a value-based method. It learns the action-value function implemented by a neural network that estimates the value of taking a discrete action at a state. At each state, we can find the best action from the action-value function as the action that gives the largest value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4b6QRJU7vY0"
      },
      "source": [
        "Stable Baselines3 (SB3) is a RL library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-eoBRjHDvsd"
      },
      "outputs": [],
      "source": [
        "%pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruIhSc1rDePx"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import random\n",
        "from typing import Tuple\n",
        "import zipfile\n",
        "\n",
        "import gymnasium as gym\n",
        "from huggingface_hub import hf_hub_download\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from stable_baselines3.dqn.dqn import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohU1Gd9UaFk2"
      },
      "source": [
        "## Download food images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = hf_hub_download(\n",
        "    repo_id=\"tttslab/spolacq_dataset\",\n",
        "    repo_type=\"dataset\",\n",
        "    filename=\"./data.zip\"\n",
        ")\n",
        "extract_dir = \"./\"\n",
        "\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "os.remove(file_path)"
      ],
      "metadata": {
        "id": "MIuqUNZj20Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXdQt35BaXfG"
      },
      "source": [
        "## Environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_XYOL10SNK9"
      },
      "source": [
        "We implement the environment using OpenAI Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNo_pr7VDePx"
      },
      "outputs": [],
      "source": [
        "class Environment(gym.Env):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        food (str): one of foods: \"apple\", \"banana\", \"carrot\", \"cherry\", \"cucumber\",\n",
        "        \"egg\", \"eggplant\", \"green_pepper\", \"hyacinth_bean\", \"kiwi_fruit\",\n",
        "        \"lemon\", \"onion\", \"orange\", \"potato\", \"sliced_bread\", \"small_cabbage\",\n",
        "        \"strawberry\", \"sweet_potato\", \"tomato\", and \"white_radish\".\n",
        "        split (str): dataset split. \"train\" or \"test\".\n",
        "    \"\"\"\n",
        "    def __init__(self, food: str = \"apple\", split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.action_space = gym.spaces.Discrete(3)  #number of foods {0, 1, 2}\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(224, 224, 3), dtype=np.uint8)  # RGB image\n",
        "\n",
        "        assert split in [\"train\", \"test\"], \"dataset split must be 'train' or 'test'\"\n",
        "\n",
        "        self.dataset = []\n",
        "        for num_of_foods in range(1, 4):\n",
        "            paths = glob(f\"data/dataset/{food}/{split}_number{num_of_foods}/group*_*.jpg\")\n",
        "            for path in paths:\n",
        "                image = Image.open(path)\n",
        "                image = np.array(image)\n",
        "\n",
        "                self.dataset.append((image, num_of_foods-1))\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed: int | None = None) -> np.ndarray:\n",
        "        state, num_of_foods = random.choice(self.dataset)\n",
        "        self.correct_answer = num_of_foods\n",
        "        return state, {}\n",
        "\n",
        "    def step(self, action) -> Tuple[np.ndarray, int, bool, dict]:\n",
        "\n",
        "        if action == self.correct_answer:\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        # Update state\n",
        "        new_state, num_of_foods = random.choice(self.dataset)\n",
        "        self.correct_answer = num_of_foods\n",
        "\n",
        "        return new_state, reward, True, True, dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7KTzwiiDePy"
      },
      "outputs": [],
      "source": [
        "food_name = \"apple\"\n",
        "env = Environment(food = food_name, split=\"train\")\n",
        "\n",
        "agent = DQN(\n",
        "    \"CnnPolicy\",\n",
        "    env,\n",
        "    buffer_size=100,\n",
        "    learning_starts=0,\n",
        "    verbose=1, #verbose=1 means printing information during training, verbose=0 means these information will not be displayed\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TxQSH0oxKw9"
      },
      "source": [
        "### initialized agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1_IEvFOxEX3"
      },
      "outputs": [],
      "source": [
        "# Initialize state\n",
        "state, info = env.reset()\n",
        "plt.imshow(state)\n",
        "\n",
        "# Agent gets an environment state and returns a decided action\n",
        "action, _ = agent.predict(state, deterministic=True)\n",
        "print(f\"Agent's answer: {action+1}\")\n",
        "\n",
        "# Environment gets an action from the agent, proceeds the time step,\n",
        "# and returns the new state and reward etc.\n",
        "state, reward, terminated, truncated, info = env.step(action)\n",
        "print(f\"Reward: {reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fwu-7zThYtd4"
      },
      "outputs": [],
      "source": [
        "agent.learn(total_timesteps=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOTXThyRUYtT"
      },
      "source": [
        "## Test agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEHRszS5O6-c"
      },
      "outputs": [],
      "source": [
        "test_env = Environment(food = food_name, split=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ6yzHzZZaSa"
      },
      "source": [
        "### Run the following cell several times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "najO4OnFQWzz"
      },
      "outputs": [],
      "source": [
        "# Initialize state\n",
        "state, info = test_env.reset()\n",
        "plt.imshow(state)\n",
        "\n",
        "# Agent gets an environment state and returns a decided action\n",
        "action, _ = agent.predict(state, deterministic=True)\n",
        "print(f\"Agent's answer: {action+1}\")\n",
        "\n",
        "# Environment gets an action from the agent, proceeds the time step,\n",
        "# and returns the new state and reward etc.\n",
        "state, reward, terminated, truncated, info = test_env.step(action)\n",
        "print(f\"Reward: {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DGa6cSycDq-"
      },
      "source": [
        "###Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWex5dgKcXdn"
      },
      "outputs": [],
      "source": [
        "#evaluate_policy() returns the mean and std of the rewards of the our trained model\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(agent, env, n_eval_episodes=20, render=False)\n",
        "env.close()\n",
        "print(f\"Mean reward: {mean_reward}\")\n",
        "print(f\"Std reward: {std_reward}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fN6O6Jc8dTPG",
        "ohU1Gd9UaFk2",
        "pXdQt35BaXfG",
        "dOTXThyRUYtT",
        "mQ6yzHzZZaSa",
        "4DGa6cSycDq-"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}