{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Generation using GPT2\n",
    "\n",
    "In this notebook, we will use GPT2 to generate code snippets. We will use the `transformers` library by Hugging Face to load the pre-trained GPT2 model and tokenizer. We will then fine-tune the model on a dataset of code snippets. Finally, we will generate code snippets using the fine-tuned model.\n",
    "\n",
    "GPT2 is a LLM that is originally pre-trained by OpenAI, and it's licensed under MIT License.<br>\n",
    "https://huggingface.co/docs/transformers/en/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation\n",
    "!pip install transformers[torch] datasets\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Code dataset\n",
    "\n",
    "[CodeSearchNet](https://huggingface.co/datasets/code_search_net) corpus is a dataset of 2 milllion (comment, code) pairs from opensource libraries hosted on GitHub. It contains code and documentation for several programming languages. Concretely, a comment is a top-level function or method comment (e.g. docstrings in Python), and code is an entire function or method. Currently, the dataset contains Python, Javascript, Ruby, Go, Java, and PHP code.\n",
    "\n",
    "In this notebook, we will use the `python` subset of the dataset for training and validating our fine-tuned GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset_train = load_dataset(\"code_search_net\", \"python\", split=\"train\")\n",
    "code_dataset_validation = load_dataset(\"code_search_net\", \"python\", split=\"validation\")\n",
    "\n",
    "print(f\"total training samples: {code_dataset_train.num_rows}\")\n",
    "print(f\"total validation samples: {code_dataset_validation.num_rows}\")\n",
    "\n",
    "print(\"we now re-sample the data to reduce the training time:\")\n",
    "# sample portion of the data\n",
    "def trainDataPct(dataset, pct=1):\n",
    "    return dataset.select(range(int(len(dataset)*pct)))\n",
    "\n",
    "training_percentage = 0.075\n",
    "validation_percentage = 0.3\n",
    "print(f\"Choosing {training_percentage*100}% of the data for training.\")\n",
    "print(f\"Choosing {validation_percentage*100}% of the data for validation.\")\n",
    "\n",
    "code_dataset_train = trainDataPct(code_dataset_train, pct=training_percentage) # 7.5% of the data = 30913 training samples\n",
    "code_dataset_validation = trainDataPct(code_dataset_validation, pct=validation_percentage)\n",
    "\n",
    "print(f\"total training samples: {code_dataset_train.num_rows}\")\n",
    "print(f\"total validation samples: {code_dataset_validation.num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a sample\n",
    "df_train = code_dataset_train.to_pandas()\n",
    "def get_sample():\n",
    "    # randomly pick index for a row to be displayed\n",
    "    idx = torch.randint(0, len(df_train), (1,)).item()\n",
    "    # Get the text descriptions according to the selected idx\n",
    "    func_doc_str = df_train[\"func_documentation_string\"][idx]\n",
    "    func_name = df_train[\"func_name\"][idx]\n",
    "    whole_func_string = df_train[\"whole_func_string\"][idx]\n",
    "    # create query and answer prompts\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out = query.format(instruction=func_doc_str) + fucntion_name.format(func_name=func_name) + \"\\n\\n\" + whole_func_string\n",
    "    print(out)\n",
    "\n",
    "get_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing prompt and encode text to token ids with tokenizer\n",
    "def preprocess_function(examples):\n",
    "    \"\"\" prompt template for training the LM \"\"\"\n",
    "    # print(examples[\"func_documentation_string\"], examples[\"func_name\"], examples[\"whole_func_string\"])\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out_list = [\n",
    "        query.format(instruction=func_doc_str) + fucntion_name.format(func_name=func_name) + \"\\n\\n\" + whole_func_string\n",
    "        for func_doc_str, func_name, whole_func_string in \n",
    "        zip(examples[\"func_documentation_string\"], examples[\"func_name\"], examples[\"whole_func_string\"])\n",
    "    ]\n",
    "    out = tokenizer(out_list)\n",
    "    return out\n",
    "\n",
    "tokenized_train = code_dataset_train.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    "    remove_columns=code_dataset_train.column_names\n",
    ")\n",
    "tokenized_validation = code_dataset_validation.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    "    remove_columns=code_dataset_validation.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after tokenized it, some samples may have the length that is longer than \n",
    "# the context size of the GPT2, which is 1024 in this case.\n",
    "# we need to group the text into smaller chunks with a specified block size that is less than 1024\n",
    "block_size = 700\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()} # sum of list is concatenation\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_train = tokenized_train.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    ")\n",
    "tokenized_validation = tokenized_validation.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"gpt2-python-code-search-test\"\n",
    "out_dir = f\"out/{run_name}\"\n",
    "batch_size = 6 # increasing batch size can speed up training too, but may require more GPU memory\n",
    "epochs = 1\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    run_name=run_name,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50, # log the training loss for every 50 steps\n",
    "    eval_steps=100, # evaluate and show the validation result every 100 steps\n",
    "    save_steps=300,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation.shuffle(42).select(range(200)), # only use 200 validation samples during training, which will be much faster\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tick = time.time()\n",
    "trainer.train()\n",
    "tock = time.time()\n",
    "print(f\"Training took {tock - tick:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset_validation = load_dataset(\"code_search_net\", \"python\", split=\"validation\")\n",
    "df_valid = code_dataset_validation.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_prompt(examples, idx=None):\n",
    "    # if no idx, we randomly select one\n",
    "    if idx is None:\n",
    "        idx = torch.randint(0, len(examples), (1,)).item()\n",
    "    print(f\"idx: {idx}\")\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out_prompt = query.format(instruction=examples[\"func_documentation_string\"][idx]) + fucntion_name.format(func_name=examples[\"func_name\"][idx])\n",
    "    return out_prompt\n",
    "\n",
    "def generate(model, tokenizer, text, device=\"cuda\", max_new_tokens=400, include_input=True):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            token_ids.to(device),\n",
    "            do_sample=True,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=1,\n",
    "            # repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    if not include_input:\n",
    "        output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    else:\n",
    "        output = tokenizer.decode(output_ids.tolist()[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Not fine-tuned GPT (the default pretrained GPT2):\n",
    "model_name = \"gpt2\"\n",
    "org_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Fine-tuned GPT on the code dataset:\n",
    "model_name = \"out/gpt2-python-code-search-test/checkpoint-300\"\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"model params: {count_parameters(trained_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with pre-trained GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_prompt(df_train, idx=None)\n",
    "# text = \"please write the function that find cosine similarity between two vectors\"\n",
    "max_new_tokens = 512\n",
    "print(f\"prompt: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\n",
    "    model=org_model, \n",
    "    tokenizer=tokenizer, \n",
    "    text=text, \n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    include_input=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with fine-tuned GPT2 model on the code dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\n",
    "    model=trained_model, \n",
    "    tokenizer=tokenizer, \n",
    "    text=text, \n",
    "    device=device,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    include_input=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance of the two models on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# evaluate function\n",
    "def get_prompt_answer(examples, idx=None):\n",
    "    # if no idx, we randomly select one\n",
    "    if idx is None:\n",
    "        idx = torch.randint(0, len(examples), (1,)).item()\n",
    "    # print(f\"idx: {idx}\")\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    prompt = query.format(instruction=examples[\"func_documentation_string\"][idx]) + fucntion_name.format(func_name=examples[\"func_name\"][idx])\n",
    "    answer = examples[\"whole_func_string\"][idx]\n",
    "    prompt += \"\\n\\n\" + answer\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\" prompt template for training the LM \"\"\"\n",
    "    query = \"\"\"Please write a function that {instruction}\"\"\"\n",
    "    fucntion_name = \"\"\"\\n\\nThe function name is: {func_name}\"\"\"\n",
    "    out = tokenizer([\n",
    "        query.format(instruction=func_doc_str) + fucntion_name.format(func_name=func_name) + \"\\n\\n\" + whole_func_string\n",
    "        for func_doc_str, func_name, whole_func_string in \n",
    "        zip(examples[\"func_documentation_string\"], examples[\"func_name\"], examples[\"whole_func_string\"])\n",
    "    ])\n",
    "    return out\n",
    "\n",
    "def evaluate(model, encodings, stride=512):\n",
    "    \"\"\" https://huggingface.co/docs/transformers/en/perplexity \"\"\"\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "    max_length = model.config.n_positions\n",
    "    print(f\"seq_len: {seq_len}, max_length: {max_length}\")\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        # set to -100 to ignore the loss over the context input\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        target_ids[:, :-trg_len] = -100 \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    cse = torch.stack(nlls).mean()\n",
    "    return cse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get the validation dataset \n",
    "num_test_samples = 100\n",
    "text = [get_prompt_answer(df_valid, idx=idx) for idx in range(num_test_samples)]\n",
    "encodings = tokenizer(\"\\n\\n\".join(text), return_tensors=\"pt\")\n",
    "models = {\n",
    "    \"GPT2\": org_model,\n",
    "    \"GPT2 /w ft\": trained_model\n",
    "}\n",
    "df_log = pd.DataFrame()\n",
    "stride_len = 512\n",
    "for model_name, model in models.items():\n",
    "    mean_cse = evaluate(model, encodings, stride_len)\n",
    "    ppl = torch.exp(mean_cse)\n",
    "    print(f\"Cross-entropy loss: {mean_cse:.4f}\")\n",
    "    print(f\"Perplexity: {ppl:.4f}\")\n",
    "    log = {\n",
    "        \"model_name\": model_name,\n",
    "        \"CSE\": mean_cse.item(),\n",
    "        \"PPL\": ppl.item()\n",
    "    }\n",
    "    df_log = pd.concat([df_log, pd.DataFrame([log])], ignore_index=True)\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare checkpoint performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"\"\"out/gpt2-python-code-search-test/{ckpt}\"\"\"\n",
    "checkpoints = [c for c in os.listdir(\"/content/out/gpt2-python-code-search-test/\") if c.startswith(\"checkpoint\")]\n",
    "\n",
    "# Sorting by numerical value in place\n",
    "checkpoints.sort(key=lambda x: int(x.split('-')[1]))\n",
    "\n",
    "# Now, print the sorted list\n",
    "print(checkpoints)\n",
    "\n",
    "num_test_samples = 100\n",
    "text = [get_prompt_answer(df_valid, idx=idx) for idx in range(num_test_samples)]\n",
    "encodings = tokenizer(\"\\n\\n\".join(text), return_tensors=\"pt\")\n",
    "\n",
    "df_log = pd.DataFrame()\n",
    "stride_len = 512\n",
    "\n",
    "for ckpt in checkpoints:\n",
    "  name = model_path.format(ckpt=ckpt)\n",
    "  trained_model = AutoModelForCausalLM.from_pretrained(name).to(device)\n",
    "  mean_cse = evaluate(trained_model, encodings, stride_len)\n",
    "  ppl = torch.exp(mean_cse)\n",
    "  print(f\"Cross-entropy loss: {mean_cse:.4f}\")\n",
    "  print(f\"Perplexity: {ppl:.4f}\")\n",
    "  log = {\n",
    "      \"ckpt_name\": f\"ckpt-{ckpt}\",\n",
    "      \"CSE\": mean_cse.item(),\n",
    "      \"PPL\": ppl.item()\n",
    "  }\n",
    "  df_log = pd.concat([df_log, pd.DataFrame([log])], ignore_index=True)\n",
    "df_log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
