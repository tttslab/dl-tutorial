{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c93ded",
   "metadata": {},
   "source": [
    "# Pytorch_llm_intro_ja.ipynb\n",
    "\n",
    "こちらは日本語向けのLLM(Large Language Model)のチュートリアルです。<br>\n",
    "ここでは、Microsoftによって学習された言語モデルを使います。<br>\n",
    "これらのモデルは、[The MIT license](https://opensource.org/license/mit/)で提供されています。\n",
    "\n",
    "このチュートリアルは、Google Colaboratoryで、GPUを使って実行されることを想定しています。\n",
    "\n",
    "This is the tutorial for LLM(Large Language Model) for Japanese.<br>\n",
    "We use the language model trained by Microsoft.<br>\n",
    "These models are under [The MIT license](https://opensource.org/license/mit/).\n",
    "\n",
    "I assume that this tutorial is used on Google Colaboratory with GPU.\n",
    "\n",
    "-----\n",
    "\n",
    "https://huggingface.co/microsoft/Phi-4-mini-instruct<br>\n",
    "https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning\n",
    "\n",
    "\n",
    "これらが、今回使用したモデルのドキュメントとなっております。\n",
    "\n",
    "These are the pages of documents for the models that we use this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183756e9",
   "metadata": {},
   "source": [
    "## ライブラリをpip installする\n",
    "\n",
    "ここでは、必要なライブラリをpipを用いてインストールします。\n",
    "\n",
    "On this part we will pip install libraries that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.50.0 accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7b598",
   "metadata": {},
   "source": [
    "## ライブラリをインポートする\n",
    "\n",
    "ここでは、必要なライブラリをインポートします。\n",
    "\n",
    "- PyTorch\n",
    "- transformers\n",
    "\n",
    "PyTorchはGoogle Colaboratoryにはデフォルトでインストールされています。<br>\n",
    "[transformers](https://github.com/huggingface/transformers)は、[HuggingFace社](https://huggingface.co/)が開発を行っている機械学習向けのライブラリです。\n",
    "\n",
    "もし現時点でGPUが使えない状態の場合は、`False`が出力されます。<br>\n",
    "もし`False`が出力されたら、GPUを使えるよう変更してください。\n",
    "\n",
    "Here we import libries that we need.\n",
    "\n",
    "- PyTorch\n",
    "- transformers\n",
    "\n",
    "Pytorch is pre-installed on Google Colaboratory.<br>\n",
    "[transformers](https://github.com/huggingface/transformers) is library for machine learning that [HuggingFace, Inc.](https://huggingface.co/) develops.\n",
    "\n",
    "If you can't use GPU now, this notebook outputs `False`.<br>\n",
    "If this cell outputs `False`, you should change setting to use GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3506307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee14d01",
   "metadata": {},
   "source": [
    "## 会話の続きを予測する\n",
    "\n",
    "まずは、会話の続きを予測するモデルを試してみましょう。\n",
    "\n",
    "First, we use the model that predicts continuation of dialogue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ab6c1",
   "metadata": {},
   "source": [
    "### TokenizerとModelを定義する\n",
    "\n",
    "ファイルを読み込んで、`tokenizer`と`model`を定義します。<br>\n",
    "少々時間が掛かります。\n",
    "\n",
    "Read files and define `tokenizer` and `model`.<br>\n",
    "It'll take a little bit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ec6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./instruct\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6100c",
   "metadata": {},
   "source": [
    "### Promptを定義する\n",
    "`prompt`を定義します。<br>\n",
    "ここで`prompt`とは、言語モデルに渡す会話のことを表します。<br>\n",
    "**ここでは内容を自由に変更することができます。**\n",
    "\n",
    "Define `prompt`.<br>\n",
    "`prompt`, that we use for now, means conversation that we pass to language model.<br>\n",
    "**Here you can change codes freely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488827c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"東京科学大学とはどのような大学ですか。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8092700",
   "metadata": {},
   "source": [
    "### 返答を予測する\n",
    "\n",
    "ここで、先ほどのパートで定義した`prompt`を処理します。<br>\n",
    "その後、返答を予測します。<br>\n",
    "少々時間が掛かります。\n",
    "\n",
    "Here `prompt` that you defined last part is processed.<br>\n",
    "Then the model predicts reply.<br>\n",
    "It'll take a little bit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "result = output[0]['generated_text']\n",
    "del output\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d6976",
   "metadata": {},
   "source": [
    "## 複雑な推論をする\n",
    "\n",
    "次に、複雑な推論をするモデルを試してみましょう。<br>\n",
    "しかし、このモデルは残念ながら英語にしか対応していません。<br>\n",
    "そこで、先ほどの推論モデルに翻訳をしてもらって、その文章をモデルに入力しましょう。<br>\n",
    "**以降、GPUメモリがあふれる可能性があります。その場合は「セッションを再起動」し、上の「TokenizerとModelを定義する」のセルだけ実行し、以下のセルを実行してください。**\n",
    "\n",
    "Next, let's try the model that does complex inference.<br>\n",
    "However, it's sorry that this model only supports English.<br>\n",
    "Therefore, we use the former model with translating the problem, and then input it to this model.<br>\n",
    "**From now on, GPU memory might overflow. If so, please restart the session and run only former \"TokenizerとModelを定義する\" cell, then run all following cells.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44374143",
   "metadata": {},
   "source": [
    "### TokenizerとModelを定義する\n",
    "\n",
    "ファイルを読み込んで、`tokenizer`と`model`を定義します。<br>\n",
    "少々時間が掛かります。\n",
    "\n",
    "Read files and define `tokenizer` and `model`.<br>\n",
    "It'll take a little bit time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98109f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-4-mini-reasoning\"\n",
    "inf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"./reasoning\",\n",
    ")\n",
    "inf_tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b8499",
   "metadata": {},
   "source": [
    "### Problemを定義する\n",
    "`problem`を定義します。<br>\n",
    "ここで`problem`とは、言語モデルに渡す問題のことを表します。<br>\n",
    "**ここでは内容を自由に変更することができます。**\n",
    "\n",
    "Define `problem`.<br>\n",
    "`problem`, that we use for now, means problem that we pass to language model.<br>\n",
    "**Here you can change codes freely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"x^2-3x+6=4を解いてください。\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edce629",
   "metadata": {},
   "source": [
    "### 解答を推論する\n",
    "\n",
    "ここで、先ほど出された問題を、いったん英語に翻訳します。<br>\n",
    "その後、推論モデルに入力して、問題の回答を出力させます。<br>\n",
    "10分ほど時間が掛かります。<br>\n",
    "\n",
    "Here the problem is translated into English.<br>\n",
    "Then we input it to the inference model and it will output the answer for the problem.<br>\n",
    "It'll take about 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a good translator.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"How can I say 「{problem}」 in English? Answer only the translated sentence.\"},\n",
    "]\n",
    "\n",
    "output = pipe(translate_messages, **generation_args)\n",
    "english_problem = output[0]['generated_text']\n",
    "del output\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"翻訳された問題: {english_problem}\")\n",
    "\n",
    "problem_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": english_problem,\n",
    "}]\n",
    "inputs = inf_tokenizer.apply_chat_template(\n",
    "    problem_messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "outputs = inf_model.generate(\n",
    "    **inputs.to(inf_model.device),\n",
    "    max_new_tokens=32768,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "outputs = inf_tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1]:])\n",
    "english_answer = outputs[0]\n",
    "del inputs, outputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "jp_translate = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a good translator.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"「{english_answer}」を日本語に翻訳してください。\"}\n",
    "]\n",
    "\n",
    "output2 = pipe(jp_translate, **generation_args)\n",
    "japanese_answer = output2[0]['generated_text']\n",
    "del output2\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"解答: {japanese_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "school_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
